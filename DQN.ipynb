{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ekgraTXl9a7d"},"outputs":[],"source":["import gym\n","import numpy as np\n","import torch as th\n","import torch.nn as nn\n","import copy\n","from random import sample\n","import matplotlib.pyplot as plt\n","\n","\n","def rollout(e, q, eps=0, T=2000):\n","    traj = []\n","    # Reset environment and get initial state\n","    x = e.reset()\n","    reward = 0\n","    for t in range(T):\n","        # Get action from policy (q network)\n","        u = q.control(th.from_numpy(x).float().unsqueeze(0), eps=eps)\n","        u = u.int().numpy().squeeze()\n","        # Execute action in the environment\n","        xp, r, d, info = e.step(u)\n","        t = dict(x=x, xp=xp, r=r, u=u, d=d, info=info)\n","        traj.append(t)\n","        reward = reward + r\n","        # Update current state\n","        x = xp\n","        # If done, terminate rollout\n","        if d:\n","            break\n","    return traj, reward\n","\n","\n","class q_t(nn.Module):\n","    def __init__(s, xdim=3, udim=1):\n","        super().__init__()\n","        \"\"\"\n","        con2d layers\n","        160 * 210 pixels to probability of 18 choices\n","        \"\"\"\n","        s.m = nn.Sequential(\n","            nn.Conv2d(3,6, kernel_size=(3,3), padding=1),\n","            nn.BatchNorm2d(6),\n","            nn.ReLU(True),\n","            nn.Conv2d(6,12, kernel_size=(3,3), padding=1),\n","            nn.BatchNorm2d(6),\n","            nn.MaxPool2d(kernel_size=2),\n","            nn.Conv2d(12,1, kernel_size=(3,3), padding=1),\n","            nn.BatchNorm2d(1),\n","            nn.MaxPool2d(kernel_size=2),\n","        )\n","\n","        s.linear_layers = nn.Sequential(\n","            nn.Linear(2080, udim)\n","        )\n","\n","    def forward(s, x):\n","        x = x.view(1, 3, 160, 210)\n","\n","        x = s.m(x)\n","\n","        x = x.view(1, -1)\n","        x = s.linear_layers(x)\n","        return x\n","\n","    def control(s, x, eps=0):\n","        # Get q values for all controls\n","        x = x.view(1, 3, 160, 210)\n","\n","        x = s.m(x)\n","\n","        x = x.view(1, -1)\n","        q = s.linear_layers(x)\n","\n","        ### TODO: XXXXXXXXXXXX\n","        # eps-greedy strategy to choose control input\n","        # note that for eps=0 you should return the correct control u\n","\n","        prob = th.tensor([eps, 1-eps])\n","        binary = th.distributions.categorical.Categorical(prob)\n","        greedy = binary.sample()\n","\n","        if greedy:\n","            u = th.argmax(q)\n","        else:\n","            length = q.size()[-1]\n","            prob_uniform = th.ones(length)\n","            prob_uniform = prob_uniform / length\n","            uniform = th.distributions.categorical.Categorical(prob_uniform)\n","            u = uniform.sample()\n","        return u\n","\n","\n","def loss(q, qc, ds):\n","    ### TODO: XXXXXXXXXXXX\n","    # 1. sample mini-batch from dataset ds\n","\n","    batch = 25\n","    sam = sample(ds, batch)\n","    f = 0\n","\n","    # 2. code up dqn with double-q trick\n","    for i in range(batch):\n","        traj = sam[i]\n","\n","        j = sample(range(len(traj)), 1)[0]\n","        j = int(j)\n","\n","        xp = traj[j]['xp']\n","        xp = th.from_numpy(xp).float().unsqueeze(0)[0]\n","        x = traj[j]['x']\n","        x = th.from_numpy(x).float().unsqueeze(0)[0]\n","        r = traj[j]['r']\n","        d = traj[j]['d']\n","        u = traj[j]['u']\n","\n","        # select the best action for the next step with original Q\n","        q_u = q(xp)\n","        up = th.argmax(q_u)\n","        # evaluate q using delayed q\n","        # print(q_u.size())\n","        q_targ = q(xp)[0][up]\n","        # compute target\n","        target = r + (1 - d) * 0.9 * q_targ\n","        # get the Q value of the current state-action pair\n","        q_now = q(x)[0][u]\n","\n","        f += (q_now - target)**2 / batch\n","\n","        '''\n","        # Using robust regression to fit the Q-function\n","        if th.abs(q_now - target) < 1:\n","            f = f + (q_now - target)**2 / 2 / batch\n","        else:\n","            f = f + (th.abs(q_now - target) - 0.5) / batch\n","        '''\n","    # 3. return the objective f\n","    return f\n","\n","\n","def evaluate(q, eps):\n","    ### TODO: XXXXXXXXXXXX\n","    # 1. create a new environment e\n","\n","    env = gym.make('Boxing-v0')\n","\n","    # 2. run the learnt q network for 100 trajectories on this new environment\n","    # to take control actions. Remember that you should not perform\n","    # epsilon-greedy exploration in the evaluation phase\n","\n","    r = 0\n","    for i in range(10):\n","        traj, rwd1 = rollout(env, q, eps=eps, T=2000)\n","        r = r + rwd1\n","    r = r / 10\n","    # 3. report the average discounted return of these 100 trajectories\n","\n","    return r"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"67JrHCtMyFiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gym==0.19.0\n","!pip install atari_py==0.2.6"],"metadata":{"id":"_UEbokbX-8Jk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__=='__main__':\n","    # Create environment\n","    e = gym.make('Boxing-v0')\n","    xdim, udim = e.observation_space.shape[0], e.action_space.n\n","\n","    # Create q network\n","    q = q_t(xdim, udim,)\n","    q = th.load('/content/dqnS1.pth')\n","    optim = th.optim.Adam(q.parameters(), lr=1e-3, weight_decay=1e-4)\n","\n","    # Dataset of trajectories\n","    ds = []\n","\n","    num_iteration = 120\n","    cumulative_reward = 0\n","    # Collect few random trajectories with eps=1\n","    for i in range(20):\n","        traj0, rwd0 = rollout(e, q, eps=1, T=2000)\n","        ds.append(traj0)\n","        cumulative_reward += rwd0\n","    for i in range(20):\n","        traj0, rwd0 = rollout(e, q, eps=0.9, T=2000)\n","        ds.append(traj0)\n","        cumulative_reward += rwd0\n","    for i in range(50):\n","        traj0, rwd0 = rollout(e, q, eps=0.7, T=2000)\n","        ds.append(traj0)\n","        cumulative_reward += rwd0\n","\n","    qc = th.load('/content/dqnSc1.pth')\n","    \n","    alpha = 0.05\n","\n","    num_rec = 20\n","    evaluation_t = np.zeros(num_rec)\n","    # evaluation_t[0] = cumulative_reward / 1000\n","    evaluation = np.zeros(num_rec)\n","    # evaluation[0] = evaluate(q)\n","    eps_value = 0.7\n","\n","    for mm in range(num_rec-1):\n","        for i in range(num_iteration):\n","\n","            q.train()\n","            t, rwd = rollout(e, q, eps=eps_value)\n","            if mm < 8:\n","              ds.append(t)\n","            else:\n","              pint = sample(range(1050), 1)[0]\n","              ds[pint] = t\n","            cumulative_reward += rwd\n","\n","            # Perform weights updates on the q network\n","            # need to call zero grad on q function to clear the gradient buffer\n","            q.zero_grad()\n","            f = loss(q, qc, ds)\n","            f.backward()\n","            optim.step()\n","\n","            # Exponential averaging for the target\n","            param_1 = q.state_dict()\n","            param_2 = qc.state_dict()\n","            for k in param_2:\n","                param_2[k] = (1 - alpha) * param_2[k] + (alpha * param_1[k])\n","            qc.load_state_dict(param_2)\n","            if i == 1:\n","                test = evaluate(q, 0)\n","\n","        evaluation[mm+1] = evaluate(q, 0)\n","        evaluation_t[mm + 1] = evaluate(q, eps_value)\n","        eps_value = max(eps_value * 0.95, 0.1)\n","        print('Logging data to plot')\n","        print(evaluation)\n","        print(evaluation_t)\n","        th.save(q, './dqnS1.pth')\n","        th.save(qc, './dqnSc1.pth')\n","\n","\n","    xaxis = np.array(range(num_rec)) * 120\n","    plt.plot(xaxis, evaluation)\n","    plt.ylabel('evaluation value')\n","    plt.xlabel('iteration')\n","    plt.savefig('./test2.jpg')\n","    plt.show()\n","    plt.plot(xaxis, evaluation_t)\n","    plt.ylabel('train value')\n","    plt.xlabel('iteration')\n","    plt.savefig('./test3.jpg')\n","    plt.show()\n","    th.save(q, './dqnS.pth')\n","    th.save(qc, './dqnSc.pth')"],"metadata":{"id":"gcFbxwdJ-Mu7"},"execution_count":null,"outputs":[]}]}