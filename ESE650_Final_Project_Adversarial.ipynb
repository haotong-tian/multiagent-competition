{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hUs5ZtgeGvxf"},"outputs":[],"source":["!pip install pettingzoo\n","!pip install multi-agent-ale-py\n","!pip install autorom\n","!AutoROM --install-dir /.\n","!AutoROM --accept-license"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v__6paeW0OlT"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","import copy\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJdFo5i-qGfZ"},"outputs":[],"source":["# %tensorflow_version 1.x\n","# import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rc0RKhF1yyRb"},"outputs":[],"source":["%tensorflow_version 1.x\n","!apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n","!pip install stable-baselines[mpi]==2.10.2\n","\n","import tensorflow as tf\n","\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9filk7yu6oJC"},"outputs":[],"source":["!pip install pygame\n","import os\n","os.environ['SDL_VIDEODRIVER']='dummy'\n","import pygame\n","pygame.display.set_mode((640,480))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IL-TtkYaHINf"},"outputs":[],"source":["import random\n","import numpy as np\n","from collections import defaultdict\n","import dill\n","\n","from pettingzoo.atari import boxing_v2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"macXxdphHgRk"},"outputs":[],"source":["env = boxing_v2.env()\n","\n","env.reset()\n","\n","env.agents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HudBiMD0IVLJ"},"outputs":[],"source":["env.action_space('first_0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_ydC3RpIZPc"},"outputs":[],"source":["env.observation_space('first_0')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9FidG6xrN5lJ"},"outputs":[],"source":["# def policy(observation, agent):\n","#     action = env.action_space(agent).sample()\n","#     return action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QNlTQ3vN80x"},"outputs":[],"source":["# env.reset()\n","# for agent in env.agent_iter(max_iter = 1000):\n","#     # print(agent)\n","#     observation, reward, done, info = env.last()\n","#     action = policy(observation, agent) if not done else None\n","#     env.step(action)\n","#     env.render() # this visualizes a single game"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBQ3MHSWBS_C"},"outputs":[],"source":["parallel_env = boxing_v2.parallel_env()\n","\n","parallel_env.reset()\n","\n","parallel_env.agents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z4cU1aSdBXcz"},"outputs":[],"source":["# observations = parallel_env.reset()\n","# max_cycles = 5000\n","# total_rew1, total_rew2 = 0, 0\n","# for step in range(max_cycles):\n","#     actions = {agent: policy(observations[agent], agent) for agent in parallel_env.agents}\n","#     print(actions)\n","#     observations, rewards, dones, infos = parallel_env.step(actions)\n","#     total_rew1 += rewards[0]\n","#     total_rew2 += rewards[1]\n","\n","# print(total_rew1, \"   \", total_rew2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60je7iUfEj5X"},"outputs":[],"source":["observation.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0XzTRdBPytd1"},"outputs":[],"source":["env.action_space(agent)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O56UhXtsytDJ"},"outputs":[],"source":["action_space_size = 18"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qFjXMmz6xIf"},"outputs":[],"source":["def preprocess_observation(obs):\n","    return obs.astype(np.uint8).reshape((1, 3, 210, 160))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jy_6Fzvoysj5"},"outputs":[],"source":["\n","class BoxingAgent(nn.Module):\n","\n","    def __init__(s, name, epsilon=0.9):\n","        super().__init__()\n","        s.eps = epsilon\n","\n","        s.conv_1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n","        s.linear_1 = nn.Linear(210 * 160 * 16, 128)\n","        s.linear_2 = nn.Linear(128, 18)\n","        s.relu = nn.ReLU()\n","\n","        s.name = name\n","\n","    def forward(s, x):\n","        \n","        x = s.relu(s.conv_1(x))\n","        # print(x.shape)\n","        x = x.reshape((x.shape[0], -1)) # Flatten the image\n","        x = s.relu(s.linear_1(x))\n","        x = s.relu(s.linear_2(x))\n","        # return torch.argmax(x)\n","        return x\n","\n","    def control(s, x, eps=0):\n","      \n","        q = s.forward(x.float())\n","\n","        q_values = q.detach().numpy().squeeze()\n","        \n","        # Choose the epsilon greedy control\n","        optim_control = np.argmax(q_values).item()\n","        \n","        # eps-greedy strategy to choose control input\n","        # note that for eps=0 you should return the correct control u\n","        if (random.random() > s.eps):\n","            u = optim_control \n","        else:\n","            u = random.choice(range(action_space_size))\n","        \n","        return u"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV6wnEiyGiyS"},"outputs":[],"source":["def rollout(env, agents, eps=0, T=200, prin = False):\n","    traj_1 = []\n","    traj_2 = []\n","    # Reset environment and get initial state\n","    env.reset()\n","    # x = {agent.name: env.observe(agent.name) for agent in agents}\n","    actions = {'first_0': 1, 'second_0': 1}\n","    x, _, _, _ = env.step(actions)\n","    \n","    for t in range(T):\n","        actions = {agent.name: agent.control(torch.from_numpy(preprocess_observation(x[agent.name]))) for agent in agents}\n","        # print(actions)\n","        obs, rwds, dones, infos = env.step(actions)\n","        \n","        tr_1 = dict(x=x[agents[0].name], xp=obs[agents[0].name], r=rwds[agents[0].name], u=actions[agents[0].name], d=dones[agents[0].name])\n","        traj_1.append(tr_1)\n","        tr_2 = dict(x=x[agents[1].name], xp=obs[agents[1].name], r=rwds[agents[1].name], u=actions[agents[1].name], d=dones[agents[1].name])\n","        traj_2.append(tr_2)\n","        \n","        x = obs\n","        \n","        if dones:\n","            break\n","        \n","    return traj_1, traj_2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2K8Tmw1MvxA"},"outputs":[],"source":["def loss(agent, target_agent, ds):\n","    \n","    gamma = 0.999\n","    batch_size = 1\n","    \n","    random_idx = np.random.choice(len(ds), batch_size)\n","    \n","    s = np.zeros((0, 3, 210, 160), dtype=np.float32)\n","    a = np.zeros((0,1), dtype=np.int64)\n","    s_prime = np.zeros((0, 3, 210, 160), dtype=np.float32)\n","    r = np.zeros((0,1), dtype=np.float32)\n","    d = np.zeros((0,1), dtype=np.float32)\n","    \n","    for i in random_idx:\n","        \n","        trajectory = ds[i]\n","\n","        for step in trajectory:\n","            # print(s.shape)\n","            # print(step['x'].shape)\n","            s = np.vstack((s, step['x'].reshape((1, 3, 210, 160))))\n","            a = np.vstack((a, step['u']))\n","            s_prime = np.vstack((s_prime, step['xp'].reshape((1, 3, 210, 160))))\n","            r = np.vstack((r, step['r']))\n","            d = np.vstack((d, step['d']))\n","    \n","    mask = torch.where(torch.from_numpy(d) == 0.0, 1.0, 0.0).float()\n","    \n","    # print(agent(torch.from_numpy(s)))\n","    # print(torch.from_numpy(a))\n","    q_pred = agent(torch.from_numpy(s)).gather(1, torch.from_numpy(a))\n","\n","\n","    with torch.no_grad():\n","        q_phi = agent(torch.from_numpy(s_prime))\n","        max_acts = q_phi.argmax(dim=1).view(-1, 1)\n","        q_target_phi = target_agent(torch.from_numpy(s_prime))\n","        q_target = q_target_phi.gather(1, max_acts)\n","        q_target = torch.from_numpy(r) + gamma * q_target.detach() * mask\n","\n","    f = (q_pred-q_target).pow(2).mean()\n","    \n","    return f"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f-KxCN9AjSs"},"outputs":[],"source":["def run():\n","\n","    # Create environment and agents\n","    env = boxing_v2.parallel_env()\n","    env.reset()\n","    agents = [BoxingAgent(name) for name in env.agents]\n","\n","    agent_targets = [copy.deepcopy(agent) for agent in agents]\n","    optimizers = [torch.optim.Adam(agent.parameters(), lr=1e-3, weight_decay=1e-4) for agent in agents]\n","\n","    # Dataset of trajectories\n","    ds_1 = []\n","    ds_2 = []\n","    counter = 0\n","    eps = 1\n","\n","    # Collect few random trajectories with eps=1\n","    for i in range(3):\n","        traj_1, traj_2 = rollout(env, agents, T=40)\n","        ds_1.append(traj_1)\n","        ds_2.append(traj_2)\n","\n","    recent_rewards_1 = []  # records the most recent 1000 training rewards\n","    eval_rewards_1 = []  # records the average evaluation rewards for every 1000 episodes\n","    train_rewards_1 = []  # records the average training rewards for every 1000 episodes\n","\n","    recent_rewards_2 = []  # records the most recent 1000 training rewards\n","    eval_rewards_2 = []  # records the average evaluation rewards for every 1000 episodes\n","    train_rewards_2 = []  # records the average training rewards for every 1000 episodes\n","\n","    for i in tqdm(range(1000)):\n","        for agent in agents:\n","          agent.train()\n","        traj_1, traj_2 = rollout(env, agents, eps=0.9, T=40)\n","        ds_1.append(traj_1)\n","        ds_2.append(traj_2)\n","\n","        dss = [ds_1, ds_2]\n","        \n","        eps = np.max((eps*0.9995, 0.01))\n","\n","        for a in range(2):\n","          curr_agent = agents[a]\n","          curr_targt = agent_targets[a]\n","          agent.zero_grad()\n","          f = loss(curr_agent, curr_targt, dss[a])\n","          f.backward()\n","          optimizers[a].step()\n","\n","          if (i % 10 == 0):\n","              for q_target_param, q_param in zip(curr_targt.parameters(), curr_agent.parameters()):\n","                  q_target_param.data.copy_(q_target_param * 0.95 + q_param.data * 0.05)\n","\n","\n","        training_reward_1 = 0\n","        training_reward_2 = 0\n","        \n","        trajectories_1 = []\n","        trajectories_2 = []\n","        for tr in range(3):\n","            trajectory_1, trajectory_2 = rollout(env, agents, eps=0.9, T=40)\n","            trajectories_1.append(trajectory_1)\n","            trajectories_2.append(trajectory_2)\n","            \n","        for trajectory in trajectories_1: \n","            training_reward_1 += sum([step['r'] for step in trajectory])\n","\n","        for trajectory in trajectories_2: \n","            training_reward_2 += sum([step['r'] for step in trajectory])\n","        \n","        train_rewards_1.append(training_reward_1 / 3)\n","        print(training_reward_1 / 3)\n","        train_rewards_2.append(training_reward_2 / 3)\n","        print(training_reward_2 / 3)\n","\n","\n","    plt.figure()\n","    plt.ylabel('reward')\n","    plt.title(\"Train_1\")\n","    plt.plot(np.arange(0, len(train_rewards_1) * 500, 500), train_rewards_1)\n","    plt.show()\n","\n","    plt.figure()\n","    plt.ylabel('reward')\n","    plt.title(\"Train_2\")\n","    plt.plot(np.arange(0, len(train_rewards_2) * 500, 500), train_rewards_2)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vD7N-5PsMwJM"},"outputs":[],"source":["run()"]},{"cell_type":"markdown","source":["#Adversarial Rollout\n","In this part, we can define any agents from the other files of our project, and use them to adversarially train against each other."],"metadata":{"id":"qtF5jtyKhdOF"}},{"cell_type":"code","source":["class ImportedAgent(nn.Module):\n","\n","    def __init__(s, name, epsilon=0.9):\n","        super().__init__()\n","        s.eps = epsilon\n","        s.name = name\n","\n","    def control(s, x):\n","      \n","        q = s.forward(x.float())\n","\n","        q_values = q.detach().numpy().squeeze()\n","        \n","        # Choose the epsilon greedy control\n","        optim_control = np.argmax(q_values).item()\n","        \n","        # eps-greedy strategy to choose control input\n","        # note that for eps=0 you should return the correct control u\n","        if (random.random() > s.eps):\n","            u = optim_control \n","        else:\n","            u = random.choice(range(action_space_size))\n","        \n","        return u"],"metadata":{"id":"8gsqj1DC9NES"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This \n","\n","def adversarial_rollout():\n","\n","    # Create environment and agents\n","    env = boxing_v2.parallel_env()\n","    env.reset()\n","\n","    # Change this initialization with the new agents\n","    # agent_1 = ImportedAgent('first_0', 0.9)\n","    # agent_1.load_state_dict(torch.load('/content/drive/MyDrive/ESE650 Final Project 4.29/dqnS.pth'))\n","\n","    # agent_2 = ImportedAgent('second_0', 0.9)\n","    # agent_2.load_state_dict(torch.load('/content/drive/MyDrive/ESE650 Final Project 4.29/dqnS.pth'))\n","\n","    agents = [agent_1, agent_2]\n","\n","    # Dataset of trajectories\n","    ds_1 = []\n","    ds_2 = []\n","    counter = 0wo\n","    eps = 1\n","\n","    # Collect few random trajectories with eps=1\n","    for i in range(100):\n","        traj_1, traj_2 = rollout(env, agents, T=40)\n","        ds_1.append(traj_1)\n","        ds_2.append(traj_2)\n","\n","    train_rewards_1 = [] \n","    train_rewards_2 = [] \n","    trajectories_1 = []\n","    trajectories_2 = []\n","\n","    for tr in range(100):\n","        trajectory_1, trajectory_2 = rollout(env, agents, eps=0.9, T=40)\n","        trajectories_1.append(trajectory_1)\n","        trajectories_2.append(trajectory_2)\n","\n","        for trajectory in trajectories_1: \n","            train_rewards_1.append(sum([step['r'] for step in trajectory]))\n","\n","        for trajectory in trajectories_2: \n","            train_rewards_2.append(sum([step['r'] for step in trajectory]))\n","\n","\n","    plt.figure()\n","    plt.ylabel('reward')\n","    plt.title(\"Train_1\")\n","    plt.plot(np.arange(0, len(train_rewards_1) * 500, 500), train_rewards_1)\n","    plt.show()\n","\n","    plt.figure()\n","    plt.ylabel('reward')\n","    plt.title(\"Train_2\")\n","    plt.plot(np.arange(0, len(train_rewards_2) * 500, 500), train_rewards_2)\n","    plt.show()"],"metadata":{"id":"NikU0sqmhH6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"TPZjr3vw-_E9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["adversarial_rollout()"],"metadata":{"id":"rGY9Qf2d-sZd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ESE650_Final_Project_Adversarial.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}